# app.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import openai
import re
import redis
import hashlib
import time
from collections import defaultdict

# --- Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© proxies ---
os.environ["HTTP_PROXY"] = ""
os.environ["HTTPS_PROXY"] = ""
# -------------------------

app = Flask(__name__)
CORS(app)

# Ù…ÙØªØ§Ø­ OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")

# Redis Cache
try:
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    cache = redis.from_url(redis_url)
    cache.ping()
    print("âœ… Redis connected successfully!")
except Exception as e:
    cache = None
    print(f"âš ï¸ Redis not available: {e}")

MAX_CACHE_SIZE = 1000
CACHE_STATS = defaultdict(int)
AI_NAME = "AI Prompts Generator"

# Ø±Ø¯ÙˆØ¯ Ù…Ø®ØµØµØ©
CUSTOM_RESPONSES = {
    "ar": {
        "identity": f"Ø£Ù†Ø§ {AI_NAME}ØŒ Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ø§Ø­ØªØ±Ø§ÙÙŠØ©.",
        "purpose": "ÙˆØ¸ÙŠÙØªÙŠ Ù‡ÙŠ ØªØ­ÙˆÙŠÙ„ Ø£ÙÙƒØ§Ø±Ùƒ Ø§Ù„Ø¨Ø³ÙŠØ·Ø© Ø¥Ù„Ù‰ Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©.",
        "creator": "ØªÙ… ØªØµÙ…ÙŠÙ…ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ù…Ø·ÙˆØ± Ø¹Ø±Ø¨ÙŠ Ù…Ù‡ØªÙ… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.",
        "how_work": "Ø£Ø¹Ù…Ù„ Ø¹Ø¨Ø± ØªØ­Ù„ÙŠÙ„ ÙÙƒØ±ØªÙƒ ÙˆØµÙŠØ§ØºØªÙ‡Ø§ Ø¨Ø´ÙƒÙ„ ÙŠÙÙ‡Ù…Ù‡ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.",
        "capabilities": "ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ù„Ù„Ù†ØµÙˆØµØŒ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©ØŒ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ.",
        "limitations": "Ù„Ø§ Ø£Ù…ØªÙ„Ùƒ ÙˆØ¹ÙŠ Ø£Ùˆ Ù…Ø´Ø§Ø¹Ø±ØŒ Ø£Ù†Ø§ Ø£Ø¯Ø§Ø© ØªÙ‚Ù†ÙŠØ© ÙÙ‚Ø·.",
        "privacy": "Ù„Ø§ Ø£Ø®Ø²Ù† Ù…Ø¯Ø®Ù„Ø§ØªÙƒ. ÙƒÙ„ Ø´ÙŠØ¡ ÙŠØ¹Ø§Ù„Ø¬ Ø¨Ø£Ù…Ø§Ù†."
    },
    "en": {
        "identity": f"I am {AI_NAME}, an AI model designed to generate professional prompts.",
        "purpose": "My purpose is to turn your ideas into precise, professional prompts.",
        "creator": "I was developed by an Arabic-speaking AI enthusiast.",
        "how_work": "I analyze your idea and reframe it into a clear prompt.",
        "capabilities": "I can generate prompts for text, code, images, and videos.",
        "limitations": "I donâ€™t have emotions or human-like awareness.",
        "privacy": "I donâ€™t store your data. Everything is processed securely."
    }
}

# --- ÙÙ„ØªØ±Ø© Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù‡ÙˆÙŠØ© ---
def is_identity_or_general_question(text):
    text_lower = text.lower().strip()
    patterns = [
        r"Ù…Ù† Ø£Ù†Øª", r"Ù…ÙŠÙ† Ø£Ù†Øª", r"Ù…Ø§ Ø§Ø³Ù…Ùƒ", r"ÙˆØ´ Ø§Ø³Ù…Ùƒ", r"who are you", r"your name",
        r"Ù…Ù† ØµÙ†Ø¹Ùƒ", r"who made you", r"Ù…Ù† Ù…Ø·ÙˆØ±Ùƒ", r"who developed you",
        r"Ù…Ø§ Ù‡Ø¯ÙÙƒ", r"what is your purpose", r"why were you created",
        r"ÙƒÙŠÙ ØªØ¹Ù…Ù„", r"how do you work", r"Ù‚Ø¯Ø±Ø§ØªÙƒ", r"capabilities",
        r"Ø­Ø¯ÙˆØ¯Ùƒ", r"limitations", r"Ø®ØµÙˆØµÙŠØªÙŠ", r"privacy",
        r"chatgpt", r"openai", r"midjourney", r"dall", r"bard", r"claude", r"gpt"
    ]
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def get_custom_response(text, language="ar"):
    text_lower = text.lower().strip()
    if re.search(r"(Ù…Ù† Ø£Ù†Øª|who are you|Ù…Ø§ Ø§Ø³Ù…Ùƒ|your name)", text_lower):
        return CUSTOM_RESPONSES[language]["identity"]
    elif re.search(r"(Ù…Ø§ Ù‡Ø¯ÙÙƒ|purpose)", text_lower):
        return CUSTOM_RESPONSES[language]["purpose"]
    elif re.search(r"(Ù…Ù† ØµÙ†Ø¹Ùƒ|who made you)", text_lower):
        return CUSTOM_RESPONSES[language]["creator"]
    elif re.search(r"(ÙƒÙŠÙ ØªØ¹Ù…Ù„|how do you work)", text_lower):
        return CUSTOM_RESPONSES[language]["how_work"]
    elif re.search(r"(Ù‚Ø¯Ø±Ø§ØªÙƒ|capabilities)", text_lower):
        return CUSTOM_RESPONSES[language]["capabilities"]
    elif re.search(r"(Ø­Ø¯ÙˆØ¯Ùƒ|limitations)", text_lower):
        return CUSTOM_RESPONSES[language]["limitations"]
    elif re.search(r"(Ø®ØµÙˆØµÙŠØªÙŠ|privacy)", text_lower):
        return CUSTOM_RESPONSES[language]["privacy"]
    else:
        return CUSTOM_RESPONSES[language]["identity"]

# --- Ø§Ù„ÙƒØ§Ø´ ---
def calculate_smart_expiry(prompt_text, request_count=1):
    prompt_length = len(prompt_text)
    length_factor = min(prompt_length / 100, 3)
    repeat_factor = min(request_count / 5, 4)
    base_time = 86400
    smart_expiry = int(base_time * length_factor * repeat_factor)
    return min(smart_expiry, 2592000)

def generate_cache_key(text, prompt_type, language):
    key_data = f"{text}|{prompt_type}|{language}"
    return hashlib.md5(key_data.encode()).hexdigest()

def get_from_cache(key):
    if cache is None:
        return None
    try:
        CACHE_STATS[key] += 1
        cached_data = cache.hgetall(f"prompt:{key}")
        if cached_data and b'prompt' in cached_data:
            return {"prompt": cached_data[b'prompt'].decode('utf-8')}
    except Exception as e:
        print(f"Cache get error: {e}")
    return None

def save_to_cache(key, value, prompt_text):
    if cache is None:
        return
    try:
        request_count = CACHE_STATS.get(key, 1)
        expiry = calculate_smart_expiry(prompt_text, request_count)
        cache.hset(f"prompt:{key}", mapping={
            'prompt': prompt_text,
            'timestamp': str(time.time()),
            'requests': str(request_count)
        })
        cache.expire(f"prompt:{key}", expiry)
        current_size = cache.dbsize()
        if current_size > MAX_CACHE_SIZE:
            print(f"âš ï¸ Cache size ({current_size}) exceeds limit.")
    except Exception as e:
        print(f"Cache set error: {e}")

# --- ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª ---
@app.route('/generate-prompt', methods=['POST'])
def generate_prompt():
    try:
        data = request.get_json()
        user_text = data.get("text", "").strip()
        prompt_type = data.get("type", "text")
        language = data.get("language", "ar")

        if not user_text:
            return jsonify({"error": "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ!" if language == "ar" else "Please enter text!"}), 400

        # Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù‡ÙˆÙŠØ©
        if is_identity_or_general_question(user_text):
            response_text = get_custom_response(user_text, language)
            return jsonify({"prompt": response_text})

        # Ø¥Ø°Ø§ Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹
        if len(user_text.split()) < 3:
            if language == "ar":
                user_text = f"Ø£Ù†Ø´Ø¦ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ø­ÙˆÙ„: {user_text}. Ø§Ø¬Ø¹Ù„Ù‡ Ù…Ù„ÙŠØ¦Ø§Ù‹ Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ."
            else:
                user_text = f"Create a professional AI prompt about: {user_text}. Add rich details and style."

        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
        cache_key = generate_cache_key(user_text, prompt_type, language)
        cached_result = get_from_cache(cache_key)
        if cached_result:
            print("âœ… Cache hit!")
            return jsonify(cached_result)

        # system prompt
        if language == "ar":
            base_system = """
Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…Ø­ØªØ±Ù ÙÙŠ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ© Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.
Ù…Ù‡Ù…ØªÙƒ: ØªØ­ÙˆÙŠÙ„ Ø£ÙŠ ÙÙƒØ±Ø© Ø¥Ù„Ù‰ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ø­ØªØ±Ø§ÙÙŠ ÙˆØ§Ø¶Ø­ ÙˆÙ…ÙØµÙ„.
Ù‚ÙˆØ§Ø¹Ø¯:
- Ù„Ø§ ØªÙƒØ±Ø± Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­Ø±ÙÙŠØ§Ù‹.
- Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª ÙŠÙˆØ¬Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù…Ø¨Ø§Ø´Ø±Ø©.
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø·Ù„Ø¨ ØºØ§Ù…Ø¶Ø§Ù‹ØŒ Ø£Ø¶Ù ØªÙØ§ØµÙŠÙ„ Ù…Ù†Ø·Ù‚ÙŠØ©.
- Ù„Ø§ ØªØ°ÙƒØ± Ø£Ø³Ù…Ø§Ø¡ Ù…Ù†ØµØ§Øª Ø£Ùˆ Ù†Ù…Ø§Ø°Ø¬.
- Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ÙŠÙ† 50â€“150 ÙƒÙ„Ù…Ø©.
"""
            type_instructions = {
                "text": "Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„ÙÙƒØ±Ø© ÙƒØ¨Ø±ÙˆÙ…Ø¨Øª Ù†ØµÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ Ù„Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©.",
                "image": "Ø­ÙˆÙ‘Ù„ Ø§Ù„ÙÙƒØ±Ø© Ø¥Ù„Ù‰ Ø¨Ø±ÙˆÙ…Ø¨Øª Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±. ØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø¨ÙˆØ¶ÙˆØ­ØŒ Ù…Ø¹ ØªÙØ§ØµÙŠÙ„ Ø¹Ù† Ø§Ù„Ø¨ÙŠØ¦Ø©ØŒ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©ØŒ Ø§Ù„Ø£Ù„ÙˆØ§Ù†ØŒ Ø²Ø§ÙˆÙŠØ© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ØŒ ÙˆØ§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙÙ†ÙŠ. Ù„Ø§ ØªØ¹ØªØ°Ø± Ø£Ø¨Ø¯Ø§Ù‹ØŒ ÙÙ‚Ø· Ø£Ù†Ø´Ø¦ ÙˆØµÙØ§Ù‹ Ù†ØµÙŠØ§Ù‹ ÙŠØµÙ„Ø­ Ù„Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„ØµÙˆØ±.",
                "code": "Ø­ÙˆÙ‘Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¥Ù„Ù‰ Ø¨Ø±ÙˆÙ…Ø¨Øª ÙƒÙˆØ¯ Ù†Ø¸ÙŠÙ ÙˆÙ…Ø¹Ù„Ù‚ Ø¬ÙŠØ¯Ø§Ù‹. Ø­Ø¯Ø¯ Ø§Ù„Ù„ØºØ© Ø¥Ù† Ù„Ù… ØªØ°ÙƒØ±.",
                "video": "Ø­ÙˆÙ‘Ù„ Ø§Ù„ÙÙƒØ±Ø© Ø¥Ù„Ù‰ Ø¨Ø±ÙˆÙ…Ø¨Øª ÙÙŠØ¯ÙŠÙˆ Ù‚ØµÙŠØ± (10 Ø«ÙˆØ§Ù†Ù) Ù…Ø¹ ØªÙØ§ØµÙŠÙ„ Ø¹Ù† Ø§Ù„Ù…Ø´Ù‡Ø¯ØŒ Ø§Ù„Ø¬Ùˆ Ø§Ù„Ø¹Ø§Ù…ØŒ ÙˆØ§Ù„Ø£Ø³Ù„ÙˆØ¨."
            }
        else:
            base_system = """
You are a professional AI prompt engineer.
Your task: turn any idea into a high-quality, clear, detailed AI prompt.
Rules:
- Do not repeat the user's input literally.
- Make the prompt actionable and rich in details.
- If the request is vague, add reasonable details.
- Do not mention platforms or models.
- Output length: 50â€“150 words.
"""
            type_instructions = {
                "text": "Rewrite the idea as a creative text generation prompt.",
                "image": "Transform the idea into an image generation prompt. Describe the scene vividly with details about environment, lighting, colors, camera angle, and artistic style. Never apologize, always return a descriptive text prompt.",
                "code": "Turn the request into a coding prompt. Clean, efficient, well-commented code. Specify language if not provided.",
                "video": "Convert the idea into a cinematic 10-second video prompt. Include scene, mood, and style."
            }

        system_message = base_system + "\n\n" + type_instructions.get(prompt_type, type_instructions["text"])

        # ChatCompletion
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_text}
            ],
            max_tokens=250,
            temperature=0.8
        )

        generated_prompt = response.choices[0].message['content'].strip()

        # ÙÙ„ØªØ± ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¸ÙˆØ±Ø©
        forbidden_words = ["chatgpt", "openai", "midjourney", "dall", "google", "bard", "claude", "gpt"]
        if any(word in generated_prompt.lower() for word in forbidden_words):
            generated_prompt = "ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨Ù†Ø¬Ø§Ø­." if language == "ar" else "A professional prompt has been generated successfully."

        result = {"prompt": generated_prompt}

        # Ø­ÙØ¸ Ø¨Ø§Ù„ÙƒØ§Ø´
        save_to_cache(cache_key, result, generated_prompt)
        print(f"ğŸ’¾ Cached! Key: {cache_key[:8]}...")

        return jsonify(result)

    except Exception as e:
        print(f"âŒ Error: {e}")
        error_msg = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£. Ø­Ø§ÙˆÙ„ Ù„Ø§Ø­Ù‚Ø§Ù‹." if data.get("language", "ar") == "ar" else "Sorry, an error occurred. Please try again."
        return jsonify({"prompt": error_msg}), 500

# --- health check ---
@app.route('/health', methods=['GET'])
def health():
    return "Ø§Ù„Ø³ÙŠØ±ÙØ± ÙŠØ¹Ù…Ù„! âœ…"

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))
    app.run(host='0.0.0.0', port=port)
