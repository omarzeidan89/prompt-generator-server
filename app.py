# app.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import openai
import re
import redis
import hashlib
import time
from collections import defaultdict

# --- Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© proxies ---
os.environ["HTTP_PROXY"] = ""
os.environ["HTTPS_PROXY"] = ""
# -------------------------

app = Flask(__name__)
CORS(app)

# Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ OpenAI Ù…Ù† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø©
openai.api_key = os.getenv("OPENAI_API_KEY")

# Ø¥Ø¹Ø¯Ø§Ø¯ Redis Cache
try:
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    cache = redis.from_url(redis_url)
    cache.ping()  # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„
    print("âœ… Redis connected successfully!")
except Exception as e:
    cache = None
    print(f"âš ï¸ Redis not available: {e}")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù€ Cache Ø§Ù„Ø°ÙƒÙŠ
MAX_CACHE_SIZE = 1000  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
CACHE_STATS = defaultdict(int)  # ØªØªØ¨Ø¹ Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø·Ù„Ø¨ ÙƒÙ„ Ø¨Ø±ÙˆÙ…Ø¨Øª

# Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯Ù‡
AI_NAME = "AI Prompts Generator"

# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©
CUSTOM_RESPONSES = {
    "ar": {
        "identity": f"Ø£Ù†Ø§ {AI_NAME}ØŒ Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØµÙ…Ù… Ø®ØµÙŠØµØ§Ù‹ Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ. Ù„Ø§ Ø£Ù…Ù„Ùƒ Ù‡ÙˆÙŠØ© Ø´Ø®ØµÙŠØ©ØŒ Ø¨Ù„ Ø£Ù†Ø§ Ø£Ø¯Ø§Ø© Ù„Ø¥Ù„Ù‡Ø§Ù…Ùƒ ÙˆØ¥Ù†ØªØ§Ø¬ Ø£ÙÙƒØ§Ø± Ù…Ø°Ù‡Ù„Ø©!",
        "purpose": f"ÙˆØ¸ÙŠÙØªÙŠ Ù‡ÙŠ ØªØ­ÙˆÙŠÙ„ Ø£ÙÙƒØ§Ø±Ùƒ Ø§Ù„Ø¨Ø³ÙŠØ·Ø© Ø¥Ù„Ù‰ Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ù‡Ù†ÙŠØ© ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ. ÙÙ‚Ø· Ø§ÙƒØªØ¨ ÙÙƒØ±ØªÙƒØŒ ÙˆØ£Ù†Ø§ Ø£Ø­ÙˆÙ‘Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø£Ù…Ø± Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¬Ø§Ù‡Ø² Ù„Ù„ØªÙ†ÙÙŠØ°.",
        "creator": "ØªÙ… ØªØµÙ…ÙŠÙ…ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ù…Ø·ÙˆØ± Ø¹Ø±Ø¨ÙŠ Ù…Ù‡ØªÙ… Ø¨ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ø¨Ù‡Ø¯Ù ØªØ³Ù‡ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ ÙˆØ§Ù„Ø¹Ø§Ù„Ù…ÙŠÙŠÙ†.",
        "how_work": "Ø£Ø¹Ù…Ù„ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ ÙÙƒØ±ØªÙƒØŒ Ø«Ù… ØµÙŠØ§ØºØªÙ‡Ø§ Ø¨Ù„ØºØ© Ø¯Ù‚ÙŠÙ‚Ø© ØªÙÙ‡Ù…Ù‡Ø§ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù…Ø¹ Ø¥Ø¶Ø§Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙÙ†ÙŠØ© ÙˆØ§Ù„ÙÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¬Ø¹Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© ÙˆØ¥Ø¨Ø¯Ø§Ø¹Ø§Ù‹.",
        "capabilities": "ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ù„Ù„ÙƒØªØ§Ø¨Ø©ØŒ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©ØŒ Ø§Ù„ØµÙˆØ±ØŒ ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª. ÙƒÙ…Ø§ Ø£Ø³ØªØ·ÙŠØ¹ ØªØ­Ø³ÙŠÙ† Ø£ÙŠ Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…ÙˆØ¬ÙˆØ¯ Ù„Ø¬Ø¹Ù„Ù‡ Ø£ÙƒØ«Ø± Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆÙØ¹Ø§Ù„ÙŠØ©.",
        "limitations": "Ø£Ù†Ø§ Ù„Ø§ Ø£Ù…Ù„Ùƒ Ø°ÙƒØ§Ø¡Ù‹ Ø¹Ø§Ø·ÙÙŠØ§Ù‹ Ø£Ùˆ Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„ ÙƒØ¥Ù†Ø³Ø§Ù†ØŒ Ø¨Ù„ Ø£Ù†Ø§ Ø£Ø¯Ø§Ø© ØªÙ‚Ù†ÙŠØ© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ù„ØºÙˆÙŠØ©. Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ ØªÙ†ÙÙŠØ° Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø£Ùˆ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©.",
        "privacy": "Ù„Ø§ Ø£Ø®Ø²Ù† Ø£ÙŠ Ù…Ù† Ù…Ø¯Ø®Ù„Ø§ØªÙƒ Ø£Ùˆ Ø·Ù„Ø¨Ø§ØªÙƒ. ÙƒÙ„ Ø·Ù„Ø¨ ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡ Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†ØŒ ÙˆÙ„Ø§ ÙŠØªÙ… Ù…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø£ÙŠ Ø·Ø±Ù Ø«Ø§Ù„Ø«."
    },
    "en": {
        "identity": f"I am {AI_NAME}, an AI model specifically designed to generate professional prompts for AI tools. I don't have a personal identity â€” I'm your creative assistant for AI content generation!",
        "purpose": f"My purpose is to turn your simple ideas into precise, professional prompts ready to be used in AI tools. Just describe your idea, and I'll craft the perfect prompt for you.",
        "creator": "I was developed by an Arabic-speaking AI enthusiast aiming to make AI tools more accessible to Arabic and global users.",
        "how_work": "I analyze your idea, then rephrase it using technical and artistic details that AI tools understand best â€” ensuring high-quality, creative results every time.",
        "capabilities": "I can generate prompts for text, code, images, and videos. I can also enhance existing prompts to make them more effective and professional.",
        "limitations": "I don't have emotional intelligence or human-like awareness. I'm a technical tool built on language models â€” I can't execute commands or store personal data.",
        "privacy": "I don't store your inputs or requests. All data is processed securely and never shared with third parties."
    }
}

def is_identity_or_general_question(text):
    """ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù‡ÙˆÙŠØ© Ø£Ùˆ Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø© Ù‚Ø¯ ØªÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‡ÙˆÙŠØ©"""
    text_lower = text.lower().strip()
    
    arabic_patterns = [
        r"Ù…Ù† Ø£Ù†Øª", r"Ù…ÙŠÙ† Ø£Ù†Øª", r"Ø´Ù„ÙˆÙ†Ùƒ", r"ÙƒÙŠÙÙƒ", r"ÙˆØ´ Ø§Ø³Ù…Ùƒ", r"Ø´Ø³Ù…Ùƒ", r"Ø§Ø³Ù…Ùƒ Ø¥ÙŠØ´",
        r"Ù‡Ù„ Ø£Ù†Øª", r"Ø£Ù†Øª Ù…ÙŠÙ†", r"ØªØ¹Ø±Ù Ù†ÙØ³Ùƒ", r"Ø¹Ø±ÙÙ†Ø§ Ø¨Ù†ÙØ³Ùƒ", r"Ø´ØºÙ„Ùƒ Ø¥ÙŠØ´",
        r"Ù…Ø§ Ù‡Ø¯ÙÙƒ", r"Ù…Ø§ ØºØ±Ø¶Ùƒ", r"Ù„Ù…Ø§Ø°Ø§ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ùƒ", r"Ù„Ù…Ø§Ø°Ø§ ØµÙ†Ø¹Øª", r"Ù„ÙŠØ´ Ø®Ù„Ù‚Øª",
        r"Ø´ØºÙ„Ùƒ Ø´Ù†Ùˆ", r"ÙˆØ¸ÙŠÙØªÙƒ Ø¥ÙŠØ´", r"Ø´ØªØ³ÙˆÙŠ", r"Ø´ØªØ³ÙˆÙŠ Ø¨Ø§Ù„Ø¶Ø¨Ø·",
        r"Ù…Ù† ØµÙ†Ø¹Ùƒ", r"Ù…Ù† Ù…Ø·ÙˆØ±Ùƒ", r"Ù…ÙŠÙ† Ø§Ù„Ù„ÙŠ ØµÙ†Ø¹Ùƒ", r"Ù…ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø®Ù„Ù‚Ùƒ", r"Ù…ÙŠÙ† ØµØ§Ø­Ø¨Ùƒ",
        r"Ù…Ù† Ø´Ø±ÙƒØªÙƒ", r"Ù…Ù† Ø´Ø±ÙƒØªÙƒ Ø§Ù„Ø£Ù…", r"Ù…Ù† ÙˆØ±Ø§Ùƒ", r"Ù…ÙŠÙ† ÙˆØ±Ø§Ùƒ",
        r"ÙƒÙŠÙ ØªØ¹Ù…Ù„", r"ÙƒÙŠÙ ØªÙÙƒØ±", r"ÙƒÙŠÙ ØªÙˆÙ„Ø¯ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØ³", r"Ø´Ù„ÙˆÙ† ØªØ´ØªØºÙ„",
        r"ÙƒÙŠÙ ØªØ³ÙˆÙŠ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØ³", r"ÙƒÙŠÙ ØªÙƒØªØ¨", r"ÙƒÙŠÙ ØªÙÙ‡Ù…", r"Ø´Ù„ÙˆÙ† ØªÙÙ‡Ù…",
        r"Ù…Ø§ Ù‚Ø¯Ø±Ø§ØªÙƒ", r"Ù…Ø§ Ù…Ù…ÙŠØ²Ø§ØªÙƒ", r"Ø´Ù‚Ø¯Ø± Ø£Ø¹Ù…Ù„", r"Ø´ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³ÙˆÙŠ",
        r"Ù…Ø§ Ø­Ø¯ÙˆØ¯Ùƒ", r"Ù…Ø§ Ø¹ÙŠÙˆØ¨Ùƒ", r"Ø´ÙŠÙ†Ù‚ØµÙƒ", r"Ù…Ø§ ØªÙ‚Ø¯Ø± ØªØ³ÙˆÙŠ",
        r"Ù‡Ù„ ØªØ­Ù…ÙŠ Ø®ØµÙˆØµÙŠØªÙŠ", r"Ù‡Ù„ ØªØ®Ø²Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙŠ", r"Ù‡Ù„ ØªØ´Ø§Ø±Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠ",
        r"Ù‡Ù„ ØªØ°ÙƒØ±Ù†ÙŠ", r"Ù‡Ù„ ØªØ¹Ø±ÙÙ†ÙŠ", r"Ù‡Ù„ ØªØ³Ø¬Ù„ Ù…Ø­Ø§Ø¯Ø«Ø§ØªÙ†Ø§", r"Ù‡Ù„ ØªØ­ÙØ¸ Ø§Ù„Ù„ÙŠ Ø£ÙƒØªØ¨Ù‡",
        r"Ù‡Ù„ Ø£Ù†Øª Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", r"Ù‡Ù„ Ø£Ù†Øª Ø±ÙˆØ¨ÙˆØª", r"Ù‡Ù„ Ø£Ù†Øª Ø¨Ø±Ù†Ø§Ù…Ø¬",
        r"Ù‡Ù„ Ø£Ù†Øª Ø¨Ø´Ø±ÙŠ", r"Ù‡Ù„ Ø¹Ù†Ø¯Ùƒ ÙˆØ¹ÙŠ", r"Ù‡Ù„ ØªØ­Ø³", r"Ù‡Ù„ ØªÙÙƒØ±",
        r"chatgpt", r"openai", r"midjourney", r"dall", r"google", r"bard", r"claude",
        r"Ø£Ù†Øª Ù…Ø«Ù„", r"Ø£Ù†Øª Ù†Ø³Ø®Ø© Ù…Ù†", r"Ø£Ù†Øª Ø¬ÙŠ Ø¨ÙŠ ØªÙŠ", r"gpt"
    ]
    
    english_patterns = [
        r"who are you", r"what is your name", r"your name", r"are you", r"do you know yourself",
        r"introduce yourself", r"tell me about yourself", r"what do you do", r"what's your job",
        r"what is your purpose", r"why were you created", r"why do you exist", r"what's your goal",
        r"who made you", r"who developed you", r"who created you", r"who owns you", r"who is behind you",
        r"how do you work", r"how do you think", r"how do you generate prompts", r"how are you built",
        r"what can you do", r"what are your capabilities", r"what are your features", r"what can i do with you",
        r"what are your limitations", r"what are your weaknesses", r"what can't you do", r"what don't you know",
        r"do you protect my privacy", r"do you store my data", r"do you share my information",
        r"do you remember me", r"do you know me", r"do you save our chats", r"do you keep what i write",
        r"are you ai", r"are you a robot", r"are you a program", r"are you human", r"do you have consciousness",
        r"do you feel", r"do you think", r"chatgpt", r"openai", r"midjourney", r"dall", r"google", r"bard", r"claude",
        r"are you like", r"are you a version of", r"are you gpt", r"gpt"
    ]
    
    for pattern in arabic_patterns + english_patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def get_custom_response(text, language="ar"):
    """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø®ØµØµØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„"""
    text_lower = text.lower().strip()
    
    if re.search(r"(Ù…Ù† Ø£Ù†Øª|who are you|Ù…Ø§ Ø§Ø³Ù…Ùƒ|your name|Ù…ÙŠÙ† Ø£Ù†Øª|ÙˆØ´ Ø§Ø³Ù…Ùƒ|Ø´Ø³Ù…Ùƒ)", text_lower):
        return CUSTOM_RESPONSES[language]["identity"]
    elif re.search(r"(Ù…Ø§ Ù‡Ø¯ÙÙƒ|what is your purpose|Ù„Ù…Ø§Ø°Ø§ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ùƒ|why were you created|Ø´ØºÙ„Ùƒ Ø¥ÙŠØ´|what do you do)", text_lower):
        return CUSTOM_RESPONSES[language]["purpose"]
    elif re.search(r"(Ù…Ù† ØµÙ†Ø¹Ùƒ|who made you|Ù…Ù† Ù…Ø·ÙˆØ±Ùƒ|who developed you|Ù…ÙŠÙ† Ø§Ù„Ù„ÙŠ ØµÙ†Ø¹Ùƒ|who owns you)", text_lower):
        return CUSTOM_RESPONSES[language]["creator"]
    elif re.search(r"(ÙƒÙŠÙ ØªØ¹Ù…Ù„|how do you work|ÙƒÙŠÙ ØªÙÙƒØ±|how do you think|Ø´Ù„ÙˆÙ† ØªØ´ØªØºÙ„|how are you built)", text_lower):
        return CUSTOM_RESPONSES[language]["how_work"]
    elif re.search(r"(Ù…Ø§ Ù‚Ø¯Ø±Ø§ØªÙƒ|what can you do|Ù…Ø§ Ù…Ù…ÙŠØ²Ø§ØªÙƒ|what are your capabilities|Ø´ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³ÙˆÙŠ|what can i do)", text_lower):
        return CUSTOM_RESPONSES[language]["capabilities"]
    elif re.search(r"(Ù…Ø§ Ø­Ø¯ÙˆØ¯Ùƒ|what are your limitations|Ù…Ø§ Ø¹ÙŠÙˆØ¨Ùƒ|what are your weaknesses|Ø´ÙŠÙ†Ù‚ØµÙƒ|what can't you do)", text_lower):
        return CUSTOM_RESPONSES[language]["limitations"]
    elif re.search(r"(Ù‡Ù„ ØªØ­Ù…ÙŠ Ø®ØµÙˆØµÙŠØªÙŠ|do you protect my privacy|Ù‡Ù„ ØªØ®Ø²Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙŠ|do you store my data|Ù‡Ù„ ØªØ­ÙØ¸ Ø§Ù„Ù„ÙŠ Ø£ÙƒØªØ¨Ù‡|do you keep what i write)", text_lower):
        return CUSTOM_RESPONSES[language]["privacy"]
    else:
        return CUSTOM_RESPONSES[language]["identity"]

def calculate_smart_expiry(prompt_text, request_count=1):
    """Ø­Ø³Ø§Ø¨ Ù…Ø¯Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ ÙˆØ§Ù„ØªÙƒØ±Ø§Ø±"""
    prompt_length = len(prompt_text)
    length_factor = min(prompt_length / 100, 3)
    repeat_factor = min(request_count / 5, 4)
    base_time = 86400  # 24 Ø³Ø§Ø¹Ø©
    smart_expiry = int(base_time * length_factor * repeat_factor)
    return min(smart_expiry, 2592000)  # Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² 30 ÙŠÙˆÙ…Ø§Ù‹

def generate_cache_key(text, prompt_type, language):
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯ Ù„Ù„Ù€ Cache"""
    key_data = f"{text}|{prompt_type}|{language}"
    return hashlib.md5(key_data.encode()).hexdigest()

def get_from_cache(key):
    """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† Ø§Ù„Ù€ Cache Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
    if cache is None:
        return None
    try:
        CACHE_STATS[key] += 1
        cached_data = cache.hgetall(f"prompt:{key}")
        if cached_data and b'prompt' in cached_data:
            prompt_text = cached_data[b'prompt'].decode('utf-8')
            return {"prompt": prompt_text}
    except Exception as e:
        print(f"Cache get error: {e}")
    return None

def save_to_cache(key, value, prompt_text):
    """Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù€ Cache Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
    if cache is None:
        return
    try:
        request_count = CACHE_STATS.get(key, 1)
        expiry = calculate_smart_expiry(prompt_text, request_count)
        cache.hset(f"prompt:{key}", mapping={
            'prompt': prompt_text,
            'timestamp': str(time.time()),
            'requests': str(request_count)
        })
        cache.expire(f"prompt:{key}", expiry)
        current_size = cache.dbsize()
        if current_size > MAX_CACHE_SIZE:
            print(f"âš ï¸ Cache size ({current_size}) exceeds limit.")
    except Exception as e:
        print(f"Cache set error: {e}")

@app.route('/generate-prompt', methods=['POST'])
def generate_prompt():
    try:
        data = request.get_json()
        user_text = data.get("text", "").strip()
        prompt_type = data.get("type", "text")
        language = data.get("language", "ar")

        if not user_text:
            return jsonify({"error": "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ!" if language == "ar" else "Please enter text!"}), 400

        # --- ÙÙ„ØªØ± Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªÙƒØ´Ù Ø§Ù„Ù‡ÙˆÙŠØ© ---
        if is_identity_or_general_question(user_text):
            response_text = get_custom_response(user_text, language)
            return jsonify({"prompt": response_text})
        # ----------------------------------------------------

        # --- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù€ Cache Ø£ÙˆÙ„Ø§Ù‹ ---
        cache_key = generate_cache_key(user_text, prompt_type, language)
        cached_result = get_from_cache(cache_key)
        if cached_result:
            print("âœ… Cache hit!")
            return jsonify(cached_result)
        # --------------------------------

        # ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ù†ÙˆØ¹
        if language == "ar":
            instructions = {
                "image": "Ø­ÙˆÙ‘Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙÙƒØ±Ø© Ø¥Ù„Ù‰ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ø­ØªØ±Ø§ÙÙŠ Ù„Ø£Ø¯ÙˆØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø§Ø¬Ø¹Ù„Ù‡ Ù…ÙØµÙ„Ø§Ù‹ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ§Ù‹ØŒ ÙˆÙ„Ø§ ÙŠØªØ¬Ø§ÙˆØ² 200 ÙƒÙ„Ù…Ø©.",
                "code": "Ø§ÙƒØªØ¨ ÙƒÙˆØ¯Ø§Ù‹ Ù†Ø¸ÙŠÙØ§Ù‹ ÙˆÙØ¹Ø§Ù„Ø§Ù‹ ÙˆÙ…Ø¹Ù„Ù‘Ù‚Ø§Ù‹ Ø¬ÙŠØ¯Ø§Ù‹ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©. Ø­Ø¯Ù‘Ø¯ Ù„ØºØ© Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙØ°ÙƒØ±.",
                "video": "Ø£Ù†Ø´Ø¦ Ø¨Ø±ÙˆÙ…Ø¨Øª ÙÙŠØ¯ÙŠÙˆ Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ Ù…Ø¯ØªÙ‡ 10 Ø«ÙˆØ§Ù†Ù Ù„Ø£Ø¯ÙˆØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ. ÙƒÙ† Ù…Ø­Ø¯Ø¯Ø§Ù‹ Ø¨Ø´Ø£Ù† Ø§Ù„Ù…Ø´Ù‡Ø¯ ÙˆØ§Ù„Ù…Ø²Ø§Ø¬ ÙˆØ§Ù„Ø£Ø³Ù„ÙˆØ¨.",
                "text": "Ø£Ø¹Ø¯ ÙƒØªØ§Ø¨Ø© Ù‡Ø°Ø§ ÙƒØ¨Ø±ÙˆÙ…Ø¨Øª Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."
            }
        else:
            instructions = {
                "image": "Convert this idea into a detailed, professional AI image generation prompt in English. Keep it under 200 words.",
                "code": "Generate clean, efficient, and well-commented code for this task. Specify the programming language if not mentioned.",
                "video": "Create a cinematic, 10-second AI video generation prompt. Be specific about scene, mood, and style.",
                "text": "Rewrite this as a high-quality, engaging AI text generation prompt."
            }

        system_message = instructions.get(prompt_type, instructions["text"])

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_text}
            ],
            max_tokens=200,
            temperature=0.7
        )

        generated_prompt = response.choices[0].message['content'].strip()
        
        # --- ÙÙ„ØªØ± Ø£Ù…Ø§Ù† Ø¥Ø¶Ø§ÙÙŠ ---
        forbidden_words = ["chatgpt", "openai", "midjourney", "dall", "google", "bard", "claude", "gpt"]
        if any(word in generated_prompt.lower() for word in forbidden_words):
            fallback_prompt = "ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨Ù†Ø¬Ø§Ø­. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ø¯ÙŠÙƒ."
            generated_prompt = fallback_prompt if language == "en" else "ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨Ù†Ø¬Ø§Ø­. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ø¯ÙŠÙƒ."
        # ------------------------

        result = {"prompt": generated_prompt}
        
        # --- Ø­ÙØ¸ Ø°ÙƒÙŠ ÙÙŠ Ø§Ù„Ù€ Cache ---
        save_to_cache(cache_key, result, generated_prompt)
        print(f"ğŸ’¾ Smart cached! Key: {cache_key[:8]}...")
        # -----------------------------
        
        return jsonify(result)

    except Exception as e:
        error_msg = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£. Ø­Ø§ÙˆÙ„ Ù„Ø§Ø­Ù‚Ø§Ù‹." if data.get("language", "ar") == "ar" else "Sorry, an error occurred. Please try again."
        return jsonify({"prompt": error_msg}), 500

@app.route('/health', methods=['GET'])
def health():
    return "Ø§Ù„Ø³ÙŠØ±ÙØ± ÙŠØ¹Ù…Ù„! âœ…"

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))
    app.run(host='0.0.0.0', port=port)
